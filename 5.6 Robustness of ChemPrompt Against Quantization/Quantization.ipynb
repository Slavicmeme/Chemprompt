{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d330a5-488e-41fe-9434-37a4ed8a338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79407678-e8f5-46ba-aa0d-59b1073aaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca7af8-3875-4433-9f68-437297512606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chemprompt.data.data_loader import DataLoader\n",
    "from Chemprompt.embeddings.full_descriptors_embedding import LLMModel\n",
    "from Chemprompt.models.sklearn_model import ScikitLearnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c071647-894b-465c-a51a-eb754b88656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA-Optimized Best Full Flags (Full Descriptors)\n",
    "flag_list = [\n",
    "    \"0000101101000000000000000001101101000000001000000000000000001000000000000000000011000000000001101000001010001000001100000010000001000001000001100\", #fold 1 \n",
    "    \"0000000110000000000000100001101001000000000100000000000000001010000000000100000000100000000000100000000100000110001111000010000011100010000000101\", #fold 2\n",
    "    \"0000100011000000000000100001110111000000100101110000000000001000000000000010000010000000000000100000000110001010001011000010000001100010000011000\", #fold 3\n",
    "    \"0000101101000000000000100001010100000000100001110000100000000000000000000010000010000000000000101000001010000100001100000010000011000011100011100\", #fold 4\n",
    "    \"0000100110000000000000100001101101000000000100010000000000001000000000000000000001000000000000100000000110000000001001000000000010100011000010100\" #fold 5\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999a60a-93a1-4574-bb16-f8987891e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_choice = \"Caco2_Wang\"\n",
    "model_repo = \"CohereLabs\"\n",
    "model_name = \"aya-expanse-8b\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b099d8e-ed01-43a7-8d2c-39b12bacb90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "loader = DataLoader()\n",
    "x, y = loader.load_dataset(dataset_choice)\n",
    "y = np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe95f5c-cc6f-4b77-b677-20eaa4129eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = [\"half\", \"full\"]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea3ccbab-1e86-4547-bc28-bbd4ed473904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/HDD1/bbq9088/miniconda3/envs/Chemprompt/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(642, 2)\n",
      "\n",
      "==============================\n",
      "Running precision mode: fp16\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5085af3ad0f7411a8490cd3529bcaae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering descriptors: 100%|██████████████████████████████████████| 642/642 [00:04<00:00, 153.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA-selected descriptors (fixed 120D subset)\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "[INFO] Fold 1: Loaded flag of length 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding SMILES: 100%|████████████████████████████████████████████| 513/513 [00:37<00:00, 13.80it/s]\n",
      "Embedding SMILES: 100%|████████████████████████████████████████████| 129/129 [00:09<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold1/regression_Predictions.csv\n",
      "Combined results saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold1/regression_CombinedResults.csv\n",
      "\n",
      "[RESULTS]\n",
      "rmse: 0.328\n",
      "r2: 0.892\n",
      "pearson: 0.945\n",
      "spearman: 0.938\n",
      "Model training completed.\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "[INFO] Fold 2: Loaded flag of length 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding SMILES: 100%|████████████████████████████████████████████| 513/513 [00:34<00:00, 14.82it/s]\n",
      "Embedding SMILES: 100%|████████████████████████████████████████████| 129/129 [00:08<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold2/regression_Predictions.csv\n",
      "Combined results saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold2/regression_CombinedResults.csv\n",
      "\n",
      "[RESULTS]\n",
      "rmse: 0.316\n",
      "r2: 0.900\n",
      "pearson: 0.953\n",
      "spearman: 0.943\n",
      "Model training completed.\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "[INFO] Fold 3: Loaded flag of length 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding SMILES: 100%|████████████████████████████████████████████| 514/514 [00:35<00:00, 14.37it/s]\n",
      "Embedding SMILES: 100%|████████████████████████████████████████████| 128/128 [00:08<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold3/regression_Predictions.csv\n",
      "Combined results saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold3/regression_CombinedResults.csv\n",
      "\n",
      "[RESULTS]\n",
      "rmse: 0.341\n",
      "r2: 0.884\n",
      "pearson: 0.943\n",
      "spearman: 0.942\n",
      "Model training completed.\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "[INFO] Fold 4: Loaded flag of length 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding SMILES: 100%|████████████████████████████████████████████| 514/514 [00:36<00:00, 13.94it/s]\n",
      "Embedding SMILES: 100%|████████████████████████████████████████████| 128/128 [00:09<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold4/regression_Predictions.csv\n",
      "Combined results saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold4/regression_CombinedResults.csv\n",
      "\n",
      "[RESULTS]\n",
      "rmse: 0.325\n",
      "r2: 0.894\n",
      "pearson: 0.946\n",
      "spearman: 0.945\n",
      "Model training completed.\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "[INFO] Fold 5: Loaded flag of length 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding SMILES: 100%|████████████████████████████████████████████| 514/514 [00:34<00:00, 14.76it/s]\n",
      "Embedding SMILES: 100%|████████████████████████████████████████████| 128/128 [00:08<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold5/regression_Predictions.csv\n",
      "Combined results saved to ./result/aya-expanse-8b_FreeSolv_fp16_20251031_055430/fold5/regression_CombinedResults.csv\n",
      "\n",
      "[RESULTS]\n",
      "rmse: 0.425\n",
      "r2: 0.819\n",
      "pearson: 0.908\n",
      "spearman: 0.919\n",
      "Model training completed.\n",
      "\n",
      "==============================\n",
      "Running precision mode: fp32\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dff80dc255e4b6c885f6cf087ee5306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 1 has a total capacity of 79.19 GiB of which 172.94 MiB is free. Process 106585 has 49.69 GiB memory in use. Including non-PyTorch memory, this process has 29.31 GiB memory in use. Of the allocated memory 28.41 GiB is allocated by PyTorch, and 131.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m save_base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./result/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_choice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_base_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m llm_model \u001b[38;5;241m=\u001b[39m \u001b[43mLLMModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_repo\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Filter descriptors globally (same as GA training setup)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m llm_model\u001b[38;5;241m.\u001b[39m_filter_descriptors(x)\n",
      "File \u001b[0;32m/HDD1/bbq9088/ChEmPrompt_Lab/Chemprompt/embeddings/full_descriptors_embedding.py:35\u001b[0m, in \u001b[0;36mLLMModel.__init__\u001b[0;34m(self, model_repo, dtype, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(ckpt, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, legacy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 필터링은 lazy-load로 진행\u001b[39;00m\n",
      "File \u001b[0;32m/HDD1/bbq9088/miniconda3/envs/Chemprompt/lib/python3.10/site-packages/transformers/modeling_utils.py:3712\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3708\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3709\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3710\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3711\u001b[0m         )\n\u001b[0;32m-> 3712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/HDD1/bbq9088/miniconda3/envs/Chemprompt/lib/python3.10/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/HDD1/bbq9088/miniconda3/envs/Chemprompt/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/HDD1/bbq9088/miniconda3/envs/Chemprompt/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/HDD1/bbq9088/miniconda3/envs/Chemprompt/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/HDD1/bbq9088/miniconda3/envs/Chemprompt/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/HDD1/bbq9088/miniconda3/envs/Chemprompt/lib/python3.10/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 1 has a total capacity of 79.19 GiB of which 172.94 MiB is free. Process 106585 has 49.69 GiB memory in use. Including non-PyTorch memory, this process has 29.31 GiB memory in use. Of the allocated memory 28.41 GiB is allocated by PyTorch, and 131.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Precision Loop (FP16 vs FP32)\n",
    "# ==========================================\n",
    "for precision in precisions:\n",
    "    model_type = \"fp16\" if precision == \"half\" else \"fp32\"\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Running precision mode: {model_type}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_base_dir = f\"./result/{model_name}_{dataset_choice}_{model_type}_{current_time}\"\n",
    "    os.makedirs(save_base_dir, exist_ok=True)\n",
    "    \n",
    "    llm_model = LLMModel(\n",
    "        model_repo=f\"{model_repo}/{model_name}\",\n",
    "        dtype=precision,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Filter descriptors globally\n",
    "    llm_model._filter_descriptors(x)\n",
    "    total_features = len(llm_model.property_names)\n",
    "    select_method = f\"GA-selected descriptors (fixed {total_features}D subset)\"\n",
    "    print(select_method)\n",
    "\n",
    "    # ==========================================\n",
    "    # K-Fold Loop\n",
    "    # ==========================================\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(x), 1):\n",
    "        print(f\"\\n===== Fold {fold_idx}/5 =====\")\n",
    "\n",
    "        x_train = [x[i] for i in train_idx]\n",
    "        x_test  = [x[i] for i in test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test  = y[test_idx]\n",
    "\n",
    "        # Restore GA-optimized flag\n",
    "        flag_str = flag_list[fold_idx - 1]\n",
    "        full_flag = np.array([c == \"1\" for c in flag_str])\n",
    "        assert len(full_flag) == total_features, (\n",
    "            f\"Flag length {len(full_flag)} != descriptor count {total_features}\"\n",
    "        )\n",
    "\n",
    "        print(f\"[INFO] Fold {fold_idx}: Loaded flag of length {len(full_flag)}\")\n",
    "\n",
    "        # ==========================================\n",
    "        # Selected Descriptor Names\n",
    "        # ==========================================\n",
    "        rdkit_names = list(llm_model._descriptor_funcs.keys())\n",
    "        prompt_names = llm_model.property_names\n",
    "        \n",
    "        assert len(rdkit_names) == len(prompt_names) == len(full_flag), \\\n",
    "            \"Descriptor name length mismatch\"\n",
    "        \n",
    "        selected_rows = [\n",
    "            {\n",
    "                \"index\": i,\n",
    "                \"rdkit_name\": rname,\n",
    "                \"prompt_name\": pname\n",
    "            }\n",
    "            for i, (keep, rname, pname) in enumerate(\n",
    "                zip(full_flag, rdkit_names, prompt_names)\n",
    "            )\n",
    "            if keep\n",
    "        ]\n",
    "        \n",
    "        print(f\"[INFO] Fold {fold_idx}: Selected {len(selected_rows)} descriptors\")\n",
    "        \n",
    "        fold_dir = os.path.join(save_base_dir, f\"fold{fold_idx}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        pd.DataFrame(selected_rows).to_csv(\n",
    "            os.path.join(fold_dir, \"selected_descriptors_detailed.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        # ==========================================\n",
    "        # Step 1. Generate LLM Embeddings\n",
    "        # ==========================================\n",
    "        X_train_emb = llm_model.get_embeddings(x_train, full_flag)\n",
    "        X_test_emb  = llm_model.get_embeddings(x_test, full_flag)\n",
    "\n",
    "        # ==========================================\n",
    "        # Step 2. Train Regression Model\n",
    "        # ==========================================\n",
    "        model = ScikitLearnModel(\"regression\", save_dir=fold_dir)\n",
    "        model.fit_and_evaluate(x_train, X_train_emb, y_train, X_test_emb, y_test)\n",
    "        print(\"Model training completed.\")\n",
    "\n",
    "\n",
    "print(\"\\nAll experiments completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
